# -*- coding: utf-8 -*-
"""train_MLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-5kaBTQqYBjjdmYCGVfbIndTZnvStWbd
"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertForMaskedLM, BertTokenizer, BertConfig, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
import pandas as pd

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForMaskedLM.from_pretrained("bert-base-uncased")

# MLMに使用するデータセットの作成
class MyCorpusDataset(Dataset):
    def __init__(self, dataframe, tokenizer):
        self.dataframe = dataframe
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        line = self.dataframe.iloc[idx]["essay"]
        tokens = self.tokenizer(line, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
        return {"input_ids": tokens["input_ids"].squeeze(), "token_type_ids": tokens["token_type_ids"].squeeze(), "attention_mask": tokens["attention_mask"].squeeze()}

data = pd.read_excel('data/training_set_rel3.xlsx')
# Show Data
data.head()

# データを必要な部分だけ切り取り
prompt_ids = [1, 2, 3, 4, 5, 6, 7, 8]
data_del = data.iloc[:, [0, 1, 2]]
data_del = data_del[data_del['essay_set'].isin(prompt_ids)]
data_del

dataset = MyCorpusDataset(data_del, tokenizer)

# Create a data collator to prepare the input for MLM
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

# Trainerの定義

training_args = TrainingArguments(
    output_dir="trained_model/",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    save_steps=10_000,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=500,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

# Save your fine-tuned model
trainer.save_pretrained("pre-trained/")